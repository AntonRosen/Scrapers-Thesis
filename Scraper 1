library(rvest)
library(xml2)
library(dplyr)
library(stringr)
library(tm)
library(readr)
library(ggplot2)


get_robotstxt("www.flashback.org")

link <- "https://www.flashback.org/t2828333"
page <- read_html(link)

times <- page %>% html_nodes(".post-heading") %>% html_text() %>% gsub('[\n\t]', '', .)
posts <- page %>% html_nodes(".post_message")
toremove <- posts %>% html_nodes(".post-bbcode-quote")
xml_remove(toremove)
posts <- posts %>% html_text(trim=TRUE)
userinfos <- page %>% html_nodes("#posts .dropdown") %>% html_text() %>% gsub('[\n\t]', '', .) 

data <- data.frame(times, posts, userinfos, stringsAsFactors = FALSE)
COPY <- data.frame(data)

data2 <- data.frame()

readUrl <- function(url) {
  out <- tryCatch(
    {   
      download.file(url, destfile = "scrapedpage.html", quiet=TRUE)
      return(1)
    },
    error=function(cond) {
      
      return(0)
    },
    warning=function(cond) {
      return(0)
    }
  )    
  return(out)
}

options(scipen = 999)

for(page_result in seq (from = 2, to = 1227)) {
  
  url <- paste0("https://www.flashback.org/t2828333p", page_result)
  
  if(readUrl(url)==1) { 
    download.file(url, destfile = "scrapedpage.html", quiet=TRUE)
    page <- read_html("scrapedpage.html")
  times <- page %>% html_nodes(".post-heading") %>% html_text() %>% gsub('[\n\t]', '', .)
  posts <- page %>% html_nodes(".post_message")
  toremove <- posts %>% html_nodes(".post-bbcode-quote")
  xml_remove(toremove)
  posts <- posts %>% html_text(trim=TRUE)
  userinfos <- page %>% html_nodes("#posts .dropdown") %>% html_text() %>% gsub('[\n\t]', '', .)
  cat("boom! ")
  data2 <- rbind(data2, data.frame(times,posts,userinfos,stringsAsFactors = FALSE))
  if (page_result %% 20 == 0) {
    closeAllConnections()
    Sys.sleep(60)
    next
  } else {
    Sys.sleep(sample(10, 1) * 0.2)
  }
    } else {er <- 1}
}
